---
title: "Predictive Liver Disease"
output: html_notebook
---

Libraries and packages

```{r}
library(ggplot2)
library(dplyr)
library(caret)
library(randomForest)
library(car)
library(gridExtra)
library(reshape2)
library(ggcorrplot)
library(pROC)  
```

Download the dataset

```{r}
data <- read.csv("/Users/asus/Desktop/R/Liver_disease_data.csv")
head(data)
summary(data)
```

As seen in the EDA part, the model has some categorical variables which assume values of 0 and 1 (for example female and male). In R, it is important to convert these variables into factors which are a specific type of data design to handle categorical data. 

```{r}
data$Diagnosis <- as.factor(data$Diagnosis)
data$Gender <- as.factor(data$Gender)
data$Smoking <- as.factor(data$Smoking)
data$Diabetes <- as.factor(data$Diabetes)
data$Hypertension <- as.factor(data$Hypertension)
```


To prevent over fitting and make sure the model generalizes well to unseen data, we are going to divide the data into test and training. 

```{r}
set.seed(123) # generate reproducible psudorandom numbers 
trainIndex <- createDataPartition(data$Diagnosis, p = 0.7, list = FALSE) # divide 70% data to train and 30% to test
train_data <- data[trainIndex, ]
test_data <- data[-trainIndex, ]
```

Train the logistic regression model given that our class Diagnostic is the dependent variable and the other variables are independent and that the outcome is binary. 

```{r}
model <- glm(Diagnosis ~ ., data = train_data, family = binomial)
summary(model)
```

Make predictions with the test data 

```{r}
pred_probs <- predict(model, newdata = test_data, type = "response")

# Convert probabilities to class labels (0 or 1)
pred_class <- ifelse(pred_probs > 0.5, 1, 0)

```

Confusion matrix to evaluate results

```{r}
conf_matrix <- table(Predicted = pred_class, Actual = test_data$Diagnosis)
print(conf_matrix)
```

```{r}
precision <- conf_matrix[2, 2] / sum(conf_matrix[2, ])
recall <- conf_matrix[2, 2] / sum(conf_matrix[, 2])
f1_score <- 2 * (precision * recall) / (precision + recall)

print(paste("Precision: ", precision))
print(paste("Recall: ", recall))
print(paste("F1 Score: ", f1_score))
```


```{r}
library(pROC)
roc_curve <- roc(test_data$Diagnosis, pred_probs)
plot(roc_curve, col = "blue", main = "ROC Curve")
auc_value <- auc(roc_curve)
print(paste("AUC: ", auc_value))
```

Evaluation metrics 

```{r}
cooks_distance <- cooks.distance(model)
plot(cooks_distance, main = "Cook's Distance")
abline(h = 4 / nrow(train_data), col = "red", lty = 2)
```



```{r}
# Null model (only the intercept)
null_model <- glm(Diagnosis ~ 1, data = train_data, family = binomial)

# Full model with all predictors
full_model <- glm(Diagnosis ~ ., data = train_data, family = binomial)

# Forward selection using AIC as the criterion
stepwise_model <- step(null_model, scope = list(lower = null_model, upper = full_model), direction = "forward")

# Summary of the final model
summary(stepwise_model)

```

```{r}


```

```{r}
# Predict probabilities on the test data
pred_probs <- predict(stepwise_model, newdata = test_data, type = "response")

# Convert probabilities to class labels (threshold = 0.5)
pred_class <- ifelse(pred_probs > 0.5, 1, 0)

# Confusion Matrix
conf_matrix <- table(Predicted = pred_class, Actual = test_data$Diagnosis)
print(conf_matrix)

# Calculate Accuracy
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
print(paste("Accuracy: ", round(accuracy * 100, 2), "%"))

# Optional: Precision, Recall, F1-Score
precision <- conf_matrix[2, 2] / sum(conf_matrix[2, ])
recall <- conf_matrix[2, 2] / sum(conf_matrix[, 2])
f1_score <- 2 * (precision * recall) / (precision + recall)

print(paste("Precision: ", round(precision * 100, 2), "%"))
print(paste("Recall: ", round(recall * 100, 2), "%"))
print(paste("F1-Score: ", round(f1_score * 100, 2), "%"))

```








