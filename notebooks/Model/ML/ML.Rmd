---
title: "Predictive Liver Disease - ML models"
output: html_notebook
---

Libraries and packages


```{r}
library(caret)
library(randomForest)
library(e1071)
library(gbm)
library(xgboost)
library(nnet)
library(naivebayes)
```


Set a seed for reproducibility, ensuring random split between training and testing datasets and random initialization weights and parameters for the ML models

```{r}
set.seed(42)
```


Pass the categorical variables to factors so R can handle them 

```{r}
data$Diagnosis <- as.factor(data$Diagnosis)
data$Gender <- as.factor(data$Gender)
data$Smoking <- as.factor(data$Smoking)
data$Diabetes <- as.factor(data$Diabetes)
data$Hypertension <- as.factor(data$Hypertension)

levels(data$Diagnosis) <- c("Class0", "Class1")
```

Divide the data into train and test in order to do evaluation metrics tests 

```{r}
trainIndex <- createDataPartition(data$Diagnosis, p = 0.7, list = FALSE)
train_data <- data[trainIndex, ]
test_data <- data[-trainIndex, ]
```

Select ML models to test our data, with some general parameters 

```{r}
models <- list(
  RandomForest = list(method = "rf", tuneGrid = expand.grid(.mtry = c(2, 3, 4))),
  SVM = list(method = "svmRadial", tuneGrid = expand.grid(.C = c(0.1, 1, 10), .sigma = c(0.01, 0.1))),
  XGBoost = list(method = "xgbTree", 
                 tuneGrid = expand.grid(
                   nrounds = c(50, 100),         # number of boosting rounds
                   max_depth = c(3, 5),          # max depth of the trees
                   eta = c(0.01, 0.1),           # learning rate
                   gamma = c(0),                 # minimum loss reduction
                   colsample_bytree = c(0.8),    # feature subsample
                   min_child_weight = c(1),      # minimum child weight
                   subsample = c(0.8)
                 )),
  KNN = list(method = "knn", tuneGrid = expand.grid(k = c(3, 5, 7))),
  NaiveBayes = list(method = "naive_bayes")
)

model_results <- list()
```

Print performance evaluation for each of the models and save the models 

```{r}
for (model_name in names(models)) {
  
  cat("\nTraining", model_name, "...\n")
  
  # Extract the method and grid for the current model
  method <- models[[model_name]]$method
  grid <- models[[model_name]]$tuneGrid
  
  # Train the model using caret::train()
  train_control <- trainControl(method = "cv", number = 3, classProbs = TRUE, summaryFunction = defaultSummary, verboseIter = FALSE)
  
  model <- train(Diagnosis ~ ., 
                 data = train_data, 
                 method = method, 
                 tuneGrid = grid, 
                 metric = "Accuracy", 
                 trControl = train_control)
  
  # Make predictions on the test data
  predictions <- predict(model, newdata = test_data)
  
  # Compute evaluation metrics
  conf_matrix <- confusionMatrix(predictions, test_data$Diagnosis)
  accuracy <- conf_matrix$overall["Accuracy"]
  precision <- conf_matrix$byClass["Pos Pred Value"]
  recall <- conf_matrix$byClass["Sensitivity"]
  f1_score <- 2 * (precision * recall) / (precision + recall)
  
  # store the results
  model_results[[model_name]] <- list(
    Model = model,
    Accuracy = accuracy,
    Precision = precision,
    Recall = recall,
    F1_Score = f1_score,
    ConfusionMatrix = conf_matrix
  )
  
  # Print only the final evaluation metrics for each model
  cat(sprintf("%s Results:\n", model_name))
  print(conf_matrix)
  cat(sprintf("  Accuracy: %.2f%%\n", accuracy * 100))
  cat(sprintf("  Precision: %.2f\n", precision))
  cat(sprintf("  Recall: %.2f\n", recall))
  cat(sprintf("  F1 Score: %.2f\n", f1_score))
}
```











